
@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03762},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/oli/Dropbox/Apps/Zotero/DL_General/2017_Vaswani et al_Attention Is All You Need/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/oli/Zotero/storage/IWG4KE23/1706.html}
}


@article{kook_herzog2020,
  title = {Deep and Interpretable Regression Models for Ordinal Outcomes},
  author = {Kook, Lucas and Herzog, Lisa and Hothorn, Torsten and D{\"u}rr, Oliver and Sick, Beate},
  year = {2022},
  journal = {Pattern Recognition},
  volume = {122},
  pages = {108263},
  publisher = {{Elsevier}},
  keywords = {Accepted,HTWG Eingetragen}
}


@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science}
}


@inproceedings{arpogaus2021probabilistic,
  title = {Probabilistic Short-Term Low-Voltage Load Forecasting Using Bernstein-Polynomial Normalizing Flows},
  booktitle = {{{ICML}} 2021, Workshop Tackling Climate Change with Machine Learning, June 26, 2021, Virtual},
  author = {Arpogaus, Marcel and Vo{\ss}, Marcus and Sick, Beate and {Nigge-Uricher}, Mark and D{\"u}rr, Oliver},
  year = {2021}
}


@misc{durr2020probabilistic,
  title = {Probabilistic Deep Learning: {{With}} Python, Keras and {{TensorFlow}} Probability},
  author = {D{\"u}rr, Oliver and Sick, Beate and Murina, Elvis},
  year = {2020},
  publisher = {{Manning Publications}}
}


@article{herzogDeepTransformationModels2022,
  title = {Deep Transformation Models for Functional Outcome Prediction after Acute Ischemic Stroke},
  author = {Herzog, Lisa and Kook, Lucas and G{\"o}tschi, Andrea and Petermann, Katrin and H{\"a}nsel, Martin and Hamann, Janne and D{\"u}rr, Oliver and Wegener, Susanne and Sick, Beate},
  year = {2022},
  month = dec,
  journal = {Biometrical Journal},
  pages = {bimj.202100379},
  issn = {0323-3847, 1521-4036},
  doi = {10.1002/bimj.202100379},
  abstract = {In many medical applications, interpretable models with high prediction performance are sought. Often, those models are required to handle semistructured data like tabular and image data. We show how to apply deep transformation models (DTMs) for distributional regression that fulfill these requirements. DTMs allow the data analyst to specify (deep) neural networks for different input modalities making them applicable to various research questions. Like statistical models, DTMs can provide interpretable effect estimates while achieving the state-of-the-art prediction performance of deep neural networks. In addition, the construction of ensembles of DTMs that retain model structure and interpretability allows quantifying epistemic and aleatoric uncertainty. In this study, we compare several DTMs, including baseline-adjusted models, trained on a semistructured data set of 407 stroke patients with the aim to predict ordinal functional outcome three months after stroke. We follow statistical principles of model-building to achieve an adequate trade-off between interpretability and flexibility while assessing the relative importance of the involved data modalities. We evaluate the models for an ordinal and dichotomized version of the outcome as used in clinical practice. We show that both tabular clinical and brain imaging data are useful for functional outcome prediction, whereas models based on tabular data only outperform those based on imaging data only. There is no substantial evidence for improved prediction when combining both data modalities. Overall, we highlight that DTMs provide a powerful, interpretable approach to analyzing semistructured data and that they have the potential to support clinical decision-making.},
  langid = {english},
  file = {/Users/oli/Zotero/storage/ELHY4N28/Herzog et al. - 2022 - Deep transformation models for functional outcome .pdf}
}


@article{durr2022bernstein,
  title = {Bernstein Flows for Flexible Posteriors in Variational Bayes},
  author = {D{\"u}rr, Oliver and H{\"o}rling, Stephan and Dold, Daniel and Kovylov, Ivonne and Sick, Beate},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.05650},
  eprint = {2202.05650},
  archiveprefix = {arxiv}
}


@misc{kookEstimatingConditionalDistributions2022,
  title = {Estimating {{Conditional Distributions}} with {{Neural Networks}} Using {{R}} Package Deeptrafo},
  author = {Kook, Lucas and Baumann, Philipp FM and D{\"u}rr, Oliver and Sick, Beate and R{\"u}gamer, David},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13665},
  eprint = {arXiv:2211.13665},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.13665},
  abstract = {Contemporary empirical applications frequently require flexible regression models for complex response types and large tabular or non-tabular, including image or text, data. Classical regression models either break down under the computational load of processing such data or require additional manual feature extraction to make these problems tractable. Here, we present deeptrafo, a package for fitting flexible regression models for conditional distributions using a tensorflow backend with numerous additional processors, such as neural networks, penalties, and smoothing splines. Package deeptrafo implements deep conditional transformation models (DCTMs) for binary, ordinal, count, survival, continuous, and time series responses, potentially with uninformative censoring. Unlike other available methods, DCTMs do not assume a parametric family of distributions for the response. Further, the data analyst may trade off interpretability and flexibility by supplying custom neural network architectures and smoothers for each term in an intuitive formula interface. We demonstrate how to set up, fit, and work with DCTMs for several response types. We further showcase how to construct ensembles of these models, evaluate models using inbuilt cross-validation, and use other convenience functions for DCTMs in several applications. Lastly, we discuss DCTMs in light of other approaches to regression with non-tabular data.},
  archiveprefix = {arxiv},
  keywords = {submitted},
  file = {/Users/oli/Dropbox/Apps/Zotero/My_Relevant_Publications/2022_Kook et al_Estimating Conditional Distributions with Neural Networks using R package/Kook et al. - 2022 - Estimating Conditional Distributions with Neural N.pdf;/Users/oli/Zotero/storage/WQPJD7EA/2211.html}
}


@inproceedings{NIPS2012_c399862d,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C.J. and Bottou, L. and Weinberger, K.Q.},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}}
}


@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {arXiv:2005.14165},
  publisher = {{arXiv}},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \textendash{} something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/oli/Zotero/storage/5UZXKV74/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

