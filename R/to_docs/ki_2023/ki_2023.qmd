---
title: "KI im Jahre 2023"
subtitle: "Was ist das, was kann es, was kann es nicht?"
author: "Prof. Oliver Dürr, Prof. Georg Umlauf und ChatGPT"
format:
  revealjs:
    embed-resources: true 
    theme: [default, ../shared_stuff/oliver_slides.scss]
    slide-number: true
    #smaller: true  
    scrollable: true
    # Geht nicht zum html exportieren 
    #chalkboard: 
    #  buttons: false
    preview-links: auto
    controls-layout: edges
    logo: ../shared_stuff/images/ios.png
    footer: <https://www.ios.htwg-konstanz.de/>
---

## Überblick

-   Einordnung

    -   Deep Learning vs. Künstliche Intelligenz

-   Prinzipielle Funktionsweise von neuronalen Netzen

    -   Einfache NN

        -   Blutdruck aus Alter, Wetter

    -   Komplexere NN (Prinzip)

    -   Training von NN

-   Grenzen der KI / Prinzipbedingte Schwächen

::: notes
KI ist so eine wichtige technology, das es wichtig ist eine ungefähre Idee der Funktionswqeise zu haben. ChatGPT wird einen ziemlichen Impact haben.
:::

# Vorbemerkung

## Der kluge Hans

![](images/image14.jpg){width="682"}

::: aside
Kluger Hans um 1895; † nach 1916
:::

::: notes
Speaker notes go here.
:::

## Tauben in der Tumorerkennung

![](images/image-1412603641.png)

10 Tauben so gut wie ein Pathologe: [youtube](https://www.youtube.com/watch?v=NsV6S8EsC0E)

## Starke vs. schwache KI {.smaller}

-   Starke / allgemeine KI (general AI)

    -   Allgemeine Intelligenz

    -   Bewusstsein "Science Fiction?"

-   Schwache KI (im folgenden nur KI)

    -   Konkreten Anwendungsproblemen

    -   Leistungen in den letzten 50 Jahren

        -   Taschenrechner, Schach

    -   Enorme Steigerungen in den letzten 10 Jahren (Deep Learning)

        -   Bilderkennung, GO, Übersetzungen, ChatGPT

##### Enorme Fortschritte in KI, aber nicht der starken

## KI (AI), Machine Learning, Deep Learning

![](images/image-1420173548.png)

Deep Learning dominates the current AI wave from \~ 2010 on

::: footer
Slide credit: <https://www.datasciencecentral.com/profiles/blogs/artificial-intelligence-vs-machine-learning-vs-deep-learning>
:::

::: notes
Evt gar nicht so gut vielleicht ist man dadurch in einer Sackgasse
:::

## Anwendungen

![](images/image-260562600.png)

## Alle NN bestehen aus Neuronen...

![](images/image-1332246839.png)

...Mache haben aber mehr Neuronen als andere

## Verschiedene Architeckturen...

![](images/image-1937300209.png)

...aber es ändert sich nicht viel vom der linearen regression zu ChatGPT

::: footer
O. Dürr, B. Sick, E. Murina Probabilistic deep learning: With python keras and tensorflow probability Manning Publications
:::

## Beispiel: ChatGPT

![](images/image-1352211049.png){width="600" height="150"}

-   Art Language Model

    -   Input X Sequence (bis zu 8000 Wörter[^1])

    -   Output Y nächstes Wort (W'keiten)

-   Typisches Netz 2020 GPT-3 175'000'000'000 Parameter

[^1]: Es sind eigendlich Tokens können auch Teile eines Worts sein

## Beispiel: Alexnet

![](images/image-203014542.png){width="600" height="150"}

-   Art Bild Klassifikation

    -   Input X Bild z.B. 1024x1024 Pixel

    -   Output Y Klasse Label (eines von 1000)

-   Typisches Netz 2012 Alex Net 60'000'000 Parameter

## Beispiel Lineare Regression

![](images/image-258068647.png){width="600" height="150"}

-   Art Lineare Regresion

    -   Input X Zahl (X=alter)

    -   Output Y Zahl (Y=Blutdruck)

-   Typisches Lineare Regression Gauss XXX

## Beispiel Fully Connected NN

![](images/image-1543239431.png){width="600" height="150"}

-   Art fully connected neural network

    -   Input X 3 Zahlen Wetter heute (1,0,0)

    -   Output 3 Zahlen Wetter morgen (Wahrscheinlichkeiten)

-   Typisches Netz Fully Connected NN

## Zusammenfassungen

![](images/image-1772161160.png)

# Training

![](images/image-2012825134.png)

## Training von NN (Beispiel Bildklassifikation)

![](images/image-1991668931.png)

## Training von ChatGPT

1.  Vorhersage des nächsten Worts

    -   Muss keine Daten labeln

    -   Trainingsdaten "Internet" (common crawl)

    -   $CO_2$ äquivalent 120 Auto für ein Jahr

2.  Finetuning als Chatbot

## Training von NN (Beispiel Blutdruck)

![](images/image-1701436344.png)

# Handson

## Vorhersage des Blutdrucks

```{R data_in_r}
dat = data.frame(matrix(ncol=2, nrow=33, c(22,131,41,139,52,128,23,128,41,171,54,105,24,116,46,137,56,145,27,106,47,111,57,141,28,114,48,115,58,153,9,123,49,133,59,157,30,117,49,128,63,155,32,122,50,183,67,176,33,99,51,130,71,172,35,121,51,133,77,178,40,147,51,144,81,217), byrow = TRUE))
colnames(dat) = c('x', 'y')
head(dat,10)
ojs_define(datt = dat)
```

```{ojs}
dat = transpose(datt)
```

Daten: Blutdruck von 33 Nordamerikanischen Frauen

Q: Welchen Blutdruck schätzen Sie für eine 75 jährige Person?

::: footer
TODO
:::

## Data

```{ojs}
function lr(pw){
 return(
  Plot.plot({
  marks: [
    Plot.dot(dat, {x: "x", y: "y"}),
    Plot.line(curve(dat), {x: "x", y: "y"})
  ],
  y: {
    label: "Blutdruck",
    domain: [80,200]
  },
  x: {
    label: "Alter",
    domain: [15,80]
  },
  width: pw,
  height: pw/1.5,
  color: "steelblue"
})
)
} 

function curve(template){
  const ages = template.map(d => d.x);
  const [minAge, maxAge] = d3.extent(ages);
  const xvals = d3.range(minAge, maxAge, (maxAge - minAge) / 100); 
  let yhat = xvals.map(x => w*x + b);
  return xvals.map((x, i) => ({x, y: yhat[i]})); //Thanks ChatGPT
}

lr(900)
```

## Training of the NN

```{ojs}
lr(700)
```

```{ojs}
//| panel: sidebar
b_view = Inputs.range([-10, 120], {label: "Intercept", step: 0.1})
w_view = Inputs.range([-5, 5], {label: "Slope", step: 0.1})
b = Generators.input(b_view)
w = Generators.input(w_view)


function color(value) {
  return  d3.scaleLinear().domain([-1, 0, 1]).range(['blue', 'gray', 'green'])(value);
}
function lw(value) {
  return d3.scaleLinear().domain([-1, 1]).range([0.1, 1])(value);
}
dot`${my_graph}`
my_graph = `digraph {
  node [fontsize=20, fontname=Arial];
  node [fixedsize=true, width=0.6, height=0.6];
  edge [arrowhead=vee, arrowsize=0.7];

  //b -> Y[penwidth=${10*lw(Math.abs(b))}, label=${b}]; //<-- Change the factor here
  b -> Y[penwidth=2, label=${b}];
  x1 -> Y[penwidth=2, label=${w}];
  
  x1 [label="x", pos="0,0"]; 
  b  [label="1", pos="0,2"]; 
  Y  [label="y", pos="1,0.8"]; 
  //x1 [label="x", pos="0,0"];
  //b  [label="1", pos="2,0"];
  //Y  [label="y", pos="1,1"];
}`
```

# Backup

## Deep Learning für Tumor Erkennung

![](images/tissue.png)

DL so gut wie PathologInnen ...

::: footer
Elvis Murina, Ruben Casanova, Hanna Honcharova-Biletska, Bart Vrugt, Alex Soltermann, Oliver Dürr, Beate Sick Deep Learning for Classification of Non-Small Cell Lung Cancer histologic subtypes. Poster ALMD 2019
:::
