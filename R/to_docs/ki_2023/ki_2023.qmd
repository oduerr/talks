---
title: "KI im Jahre 2023"
subtitle: "Was ist das, was kann es, was kann es nicht?"
author: "Prof. Oliver Dürr, Prof. Georg Umlauf"

format: 
  revealjs:
    #faster rendering if not embedded
    embed-resources: true 
    theme: [default, ../shared_stuff/oliver_slides.scss]
    slide-number: true
    #smaller: true  #Funktioniert nicht so gut 
    scrollable: true
    # Geht nicht zum html exportieren 
    #chalkboard: 
    #  buttons: true
    preview-links: auto
    controls-layout: edges
    logo: ../shared_stuff/images/ios.png
    footer: <https://github.com/oduerr/stat>
bibliography: references.bib
---

<!-- 
```{=html}

Fuer die Webseite: 
embed-resources: true
und ohne chalkboard
Für die Presentation
 #chalkboard:
    #  buttons: true
```
-->

## Überblick

-   Einordnung

    -   Deep Learning vs. Künstliche Intelligenz

-   Prinzipielle Funktionsweise von neuronalen Netzen

    -   Einfache NN im Detail

-   Grenzen der KI

    -   Fokus auf Transparenz

::: notes
KI ist so eine wichtige technology, das es wichtig ist eine ungefähre Idee der Funktionswqeise zu haben. ChatGPT wird einen ziemlichen Impact haben.
:::

# Vorbemerkung

## Der kluge Hans

![](images/image14.jpg){width="682"}

::: aside
Kluger Hans um 1895; † nach 1916
:::

::: notes
Ein Beispiel dafür das wir Menschen schnell irgendwas Inteligenz zuschreiben.
:::

## Tauben in der Tumorerkennung

![](images/image-1412603641.png)

10 Tauben so gut wie ein Pathologe: [youtube](https://www.youtube.com/watch?v=NsV6S8EsC0E) 

## KI (AI), Machine Learning, Deep Learning

![](images/image-1420173548.png)

Deep Learning ist der Treiber der jetzigen KI Welle (von 2012 an).

::: footer
Slide credit: [artificial-intelligence-vs-machine-learning-vs-deep-learning](https://www.datasciencecentral.com/profiles/blogs/artificial-intelligence-vs-machine-learning-vs-deep-learning)
:::

::: notes
Alles schwache AI. Evt gar nicht so gut vielleicht ist man dadurch in einer Sackgasse
:::

## Anwendungen der (schachen) KI

![](images/image-260562600.png)

## (Künstliche) Neuronale Netze

![](images/image-941884643.png)

::: notes
Wenn wir NN sagen meinen wir immer die Biologischen
:::

## Alle NN bestehen aus Neuronen...

![](images/image-1332246839.png)

...Mache haben aber mehr Neuronen als andere.

# Beispiele für Neuronale Netze
Wichtig ist was hinten rauskommt (und vorne reingeht)

## Beispiel: ChatGPT

![](images/image-1352211049.png){width="600" height="150"}

-   Architektur: "Language Model"

    -   **Input**: X Sequenz (bis zu 8000 Wörter[^1])

    -   **Output**: Y Nächstes Wort (W'keiten)

-   Typisches Netz **2020** GPT-3 175'000'000'000 Parameter (Neuronen) @brownLanguageModelsAre2020

[^1]: Es sind eigendlich Tokens können auch Teile eines Worts sein

## Beispiel: Alexnet

![](images/image-203014542.png){width="600" height="150"}

-   Architektur: Bild Klassifikation

    -   **Input** X Bild z.B. 1024x1024 Pixel

    -   **Output** Y Klasse Label (eines von 1000)

-   Typisches Netz **2012** Alex Net 60'000'000 Parameter @NIPS2012_c399862d

<!-- ## Beispiel: Fully Connected NN -->

<!-- ![](images/image-1543239431.png){width="600" height="150"} -->

<!-- -   Architektur: "fully connected neural network" -->

<!--     -   **Input** X 3 Zahlen Wetter heute (1,0,0) -->

<!--     -   **Output** 3 Zahlen Wetter morgen (Wahrscheinlichkeiten) -->

<!-- -   Typisches Netz Fully Connected NN -->

## Beispiel: GaussNet (Lineare Regression)

![](images/image-258068647.png){width="600" height="150"}

-   Architektur Lineare Regresion

    -   **Input** X Zahl (X=alter)

    -   **Output** Y Zahl (Y=Blutdruck)

-   Typisches Lineare Regression Gauss (unpublished \~ **1795**), Legendre (1805)

::: notes
"Our principle, which we have made use of since 1795, has lately been published by Legendre...."
:::

# Training von NN

![](images/image-2012825134.png)

::: footer
Slide credit: @durr2020probabilistic
:::

## Training von NN (Beispiel Bildklassifikation)

![](images/image-1991668931.png)

# Einfache Netzwerke (im Detail)

## NN 1: Vorhersage des Blutdrucks

**Trainingsdaten**: Blutdruck von 33 Nordamerikanischen Frauen (ersten 10)

```{R data_in_r}
dat = data.frame(matrix(ncol=2, nrow=33, c(22,131,41,139,52,128,23,128,41,171,54,105,24,116,46,137,56,145,27,106,47,111,57,141,28,114,48,115,58,153,9,123,49,133,59,157,30,117,49,128,63,155,32,122,50,183,67,176,33,99,51,130,71,172,35,121,51,133,77,178,40,147,51,144,81,217), byrow = TRUE))
colnames(dat) = c('x', 'y')
head(dat,10)
ojs_define(datt = dat)
```

```{ojs}
dat = transpose(datt)
```

**Aufgabe**: Blutdruck für eine 75 jährige Frau (gegeben den Trainingsdaten)?

::: footer
Source: Colton. Statistics in Medicine (1974)
:::

## Data {auto-animate="true," auto-animate-easing="ease-in-out"}

```{ojs}
function lr(pw){
 return(
  Plot.plot({
  marks: [
    Plot.dot(dat, {x: "x", y: "y"}),
    Plot.line(curve(dat), {x: "x", y: "y"})
  ],
  y: {
    label: "Blutdruck",
    domain: [80,200]
  },
  x: {
    label: "Alter",
    domain: [15,80]
  },
  width: pw,
  height: pw/1.5,
  color: "steelblue"
})
)
} 

function curve(template){
  const ages = template.map(d => d.x);
  const [minAge, maxAge] = d3.extent(ages);
  const xvals = d3.range(minAge, maxAge, (maxAge - minAge) / 100); 
  let yhat = xvals.map(x => w*x + b);
  return xvals.map((x, i) => ({x, y: yhat[i]})); //Thanks ChatGPT
}

lr(800)
```

Idee?

## Lineare Regression {auto-animate="true," auto-animate-easing="ease-in-out"}

```{ojs}
lr(550)
```

Modell mit 2 Parametern $w,b$, welches für jedes gegebenes Alter ($x$) den Blutdruck($y$) vorhersagt:

$$
y = w \cdot x + b
$$

```{ojs}
//| panel: sidebar
viewof b = Inputs.range([-10, 120], {label: "Intercept", step: 0.1})
viewof w = Inputs.range([-5, 5], {label: "Slope", step: 0.1})
html`<br>y = ${w} &middot; x + ${b}<br>`
```

## 

![](images/image-1038034802.png)

## Lineare Regression as Neuronales Netz {auto-animate="true," auto-animate-easing="ease-in-out"}

```{ojs}
//Redoing the plot
lr(350)
```

```{ojs}
//| panel: sidebar
Inputs.bind(Inputs.range([-10, 120]), viewof b)
Inputs.bind(Inputs.range([-5, 5]), viewof w)
html`<br>y = ${w} &middot; x + ${b}<br>`

function color(value) {
  return  d3.scaleLinear().domain([-1, 0, 1]).range(['blue', 'gray', 'green'])(value);
}
function lw(value) {
  return d3.scaleLinear().domain([-1, 1]).range([0.1, 1])(value);
}

dot`${my_graph}`
my_graph = `digraph {
  node [fontsize=20, fontname=Arial];
  node [fixedsize=true, width=0.6, height=0.6];
  edge [arrowhead=vee, arrowsize=0.7];

  //b -> Y[penwidth=${10*lw(Math.abs(b))}, label=${b}]; //<-- Change the factor here
  b -> Y[penwidth=2, label=${b}];
  x1 -> Y[penwidth=2, label=${w}];
  
  x1 [label="x", pos="0,0"]; 
  b  [label="1", pos="0,2"]; 
  Y  [label="y", pos="1,0.8"]; 
  //x1 [label="x", pos="0,0"];
  //b  [label="1", pos="2,0"];
  //Y  [label="y", pos="1,1"];
}`
```

-   Gewichte der Neuronen:
    -   Parameter des Modells (hier 2 Werte)
-   Training:
    -   Bestimmen der opt. Gewichte für Trainingsdaten
-   Vorhersage:
    -   Berechnung von $y$ aus $x$ (forwardpass)
-   Interpretierbar:
    -   Pro Jahr steigt Blutdruck um \${w} Punkte

::: footer
:::

::: notes
Training am Slider Ziehen Durch das Netz gehen
:::

# Grosse Netzwerke

## Training großer Netze

-   Lineare regression hat 2 parameter (slider)

-   Grosse Netze haben Mio/Mrd parameter

-   Training Minimierung einer Verlustfunktion auf den Trainingsdaten

    -   Trainingsprinzip: Wie gut sagt das Model die Daten vorher (Maximum Likelihood Prinzip)

    -   Algorithmus: Backpropagation (Rummelhard)

    -   Training von großen Netzten große technoligische Herausforderung

::: callout-note
## Vorhersagen nur so gut wie Trainingsbeispiele!
:::

::: footer
@rumelhartLearningRepresentationsBackpropagating1986
:::

::: notes
Chat-GPT Grundlage GTP3 mehrere Monate auf 1000 Rechnern
:::

## ChatGTP

![](images/image-1990935013.png)

## Principles of ChatGPT

-   Transformer Architecture / self attention

    -   Spezielle Art neuronen zu verbinden: @vaswaniAttentionAllYou2017

-   "trained with maximum likelihood"

    -   "Normales Training"

-   Generative Language Model \[...\] predict next token in a sequence of tokens

    -   Sagt nächstes Wort vorraus

::: notes
Maximum Likelihood Parameter des Models werden so eingestellt das beobachtete Trainings
:::

## Generating Text (You)

![](images/image-2107565836.png)

Wahrscheinlichkeiten (des Sprachmodels) für das nächste Wort

-   trinken (70%)

-   bieremoji (15%)

-   oder (10%)

-   sonstige kummuliert (5%)

## Generating Text (ChatGPT)

![](images/image-1990935013.png){fig-align="center"}

::: incremental
-   Steps 1: Describe your technology in one sentence. -\> I

-   Steps 2: Describe your technology in one sentence. I -\> am

-   Steps 3: Describe your technology in one sentence. I am -\> a

-   Steps 4: Describe your technology in one sentence. I am a -\> generative

-   ...

-   Step 36 Describe your technology in one sentence. I am a ... new text. -\> \<END\>
:::

Beim Text wird das nächste Wort proportional zur Wahrscheinlichkeit ausgewürfelt.

## Zufälliges Auswählen: Beispiel 1

![](images/image-1958402932.png)

## Zufälliges Auswählen: Beispiel 2

![](images/image-45744430.png)

## Training von ChatGPT

1.  Vorhersage des nächsten Worts

    -   Muss keine Daten labeln

    -   Trainingsdaten "Internet" (common crawl)

    -   CO2 äquivalent 120 Auto für ein Jahr

2.  Finetuning als Chatbot

# Zwischen Fazit

-   Deep Learning NN Modelle sind parametrische Modelle

![](images/X-N-Y.svg){width="300"}

-   Einfachstes Beispiel lineare Regression

-   Die Parameter werden an Trainingsdaten gefittet

Oder prägnanter...

## 

![](images/image-2019671148.png)

\<\<All the impressive achievements of deep learning amount to just curve fitting\>\>

Juda Pearl, 2018

# Transparenz von NN

## Transparenz (im nachhinein)

-   Wichtigkeit von bestimmten Eingangsdaten für die Entscheidungen.

    ![](images/image-899155139.png)

::: footer
https://arxiv.org/pdf/1311.2901.pdf
:::

## Transparenz (Design der Netze)

-   Vorhersage vs. Interpretation

    -   Einfach Modelle, wie lineare Regression können interpretiert werden.

    -   Komplexe Modelle oft in den Vorhersagen besser.

    -   Kombination von einfachen interpretierbaren mit komplexen Modellen

-   BMBF Projekts "[DeepDoubt](https://www.softwaresysteme.dlr-pt.de/de/ki-erkl-rbarkeit-und-transparenz.php)"

::: columns
::: {.column width="60%"}
![](images/image-1108683085.png)
:::

::: {.column width="40%"}
-   Bild Daten (hier Hautkrebs) mit tiefen NN
-   Alter mit linearer Regression
:::
:::

::: footer
@herzogDeepTransformationModels2022, @durr2022bernstein, @kookEstimatingConditionalDistributions2022
:::

## Statistische Natur von KI

![](images/image-955836853.png)

## Statistik

![](images/image-1185358057.png)

Fussvergrößerung ==\> mehr Gehalt?

## Statistik vs. Kausalität {auto-animate="true," auto-animate-easing="ease-in-out"}

::: columns
::: {.column width="60%"}
![](images/image-1185358057.png)
:::

::: {.column width="40%"}
![](images/schuh_einkommen.png)
:::
:::

## KI ist nicht kausal (2023)

![](images/image-747377808.png)

::: notes
-   Vielleicht befindet sich KI Forschung auch in einer Sackgasse Wichtige Fragestellungen sind Kausal:
:::

## Fazit

KI ist ein hilfreiches Werkzeug:

-   Code für Animationen wurde mit ChatGPT entwickelt

-   Viele Nüzliche Anwendungen Medizin, Stromvorhersage[^2],...

[^2]: @arpogaus2021probabilistic

Allerdings:

-   Auch wenn es schwerfällt zu glauben: kein inneres Verständniss
-   Biased
    -   Nur so gut wie Trainingsdaten
-   Transparent
    -   Maximal im statistischen Sinne

::: notes
Ich bin auch meist freundlich zu ChatGPT
:::

## Danke für Ihre Aufmerksamkeit

## Literature
